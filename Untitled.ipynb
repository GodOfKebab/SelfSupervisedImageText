{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b77b2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############loading SpacyTokenizer###########\n",
      "Using GPU device 0\n",
      "Expt: experiments/mylogg/2023-12-06T16:14:03.3787216h6rbkcw\n",
      "RunID: 2023-12-06T16:14:03.378721\n",
      "current directory: /app\n",
      "Load filenames from: data/birds/train/filenames.pickle (8855)\n",
      "[BirdsDataset] (0.00%=0/11788) Reading Birds Caption Files... \n",
      "[BirdsDataset] (9.99%=1178/11788) Reading Birds Caption Files... \n",
      "[BirdsDataset] (19.99%=2356/11788) Reading Birds Caption Files... \n",
      "[BirdsDataset] (29.98%=3534/11788) Reading Birds Caption Files... \n",
      "[BirdsDataset] (39.97%=4712/11788) Reading Birds Caption Files... \n",
      "[BirdsDataset] (49.97%=5890/11788) Reading Birds Caption Files... \n",
      "[BirdsDataset] (59.96%=7068/11788) Reading Birds Caption Files... \n",
      "[BirdsDataset] (69.95%=8246/11788) Reading Birds Caption Files... \n",
      "[BirdsDataset] (79.95%=9424/11788) Reading Birds Caption Files... \n",
      "[BirdsDataset] (89.94%=10602/11788) Reading Birds Caption Files... \n",
      "[BirdsDataset] (99.93%=11780/11788) Reading Birds Caption Files... \n",
      "[BirdsDataset] (0.00%=0/117880) Calculating Max Token Length... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BirdsDataset] (10.00%=11788/117880) Calculating Max Token Length... \n",
      "[BirdsDataset] (20.00%=23576/117880) Calculating Max Token Length... \n",
      "[BirdsDataset] (30.00%=35364/117880) Calculating Max Token Length... \n",
      "[BirdsDataset] (40.00%=47152/117880) Calculating Max Token Length... \n",
      "[BirdsDataset] (50.00%=58940/117880) Calculating Max Token Length... \n",
      "[BirdsDataset] (60.00%=70728/117880) Calculating Max Token Length... \n",
      "[BirdsDataset] (70.00%=82516/117880) Calculating Max Token Length... \n",
      "[BirdsDataset] (80.00%=94304/117880) Calculating Max Token Length... \n",
      "[BirdsDataset] (90.00%=106092/117880) Calculating Max Token Length... \n",
      "[BirdsDataset] number of captions in birds dataset 88550\n",
      "[BirdsDataset] Built vocab!\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/birds/val/filenames.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 112>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m     text_datasets \u001b[38;5;241m=\u001b[39m {x: FlowerstextDataset(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir), transform\u001b[38;5;241m=\u001b[39mdata_transforms, split\u001b[38;5;241m=\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m data_set_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 115\u001b[0m     text_datasets \u001b[38;5;241m=\u001b[39m {x: BirdstextDataset(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir), transform\u001b[38;5;241m=\u001b[39mdata_transforms, split\u001b[38;5;241m=\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# Create training and validation dataloaders\u001b[39;00m\n\u001b[1;32m    118\u001b[0m ds \u001b[38;5;241m=\u001b[39m text_datasets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    113\u001b[0m     text_datasets \u001b[38;5;241m=\u001b[39m {x: FlowerstextDataset(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir), transform\u001b[38;5;241m=\u001b[39mdata_transforms, split\u001b[38;5;241m=\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m data_set_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 115\u001b[0m     text_datasets \u001b[38;5;241m=\u001b[39m {x: \u001b[43mBirdstextDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_transforms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# Create training and validation dataloaders\u001b[39;00m\n\u001b[1;32m    118\u001b[0m ds \u001b[38;5;241m=\u001b[39m text_datasets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/app/data/datasets1.py:262\u001b[0m, in \u001b[0;36mBirdstextDataset.__init__\u001b[0;34m(self, rootdir, transform, vocab_builder, split, img_format)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(rootdir, split)\n\u001b[1;32m    261\u001b[0m filepath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilenames.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    263\u001b[0m     filenames \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoad filenames from: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (filepath, \u001b[38;5;28mlen\u001b[39m(filenames)))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/birds/val/filenames.pickle'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torchvision\n",
    "import time\n",
    "import sys\n",
    "from data.datasets1 import ShapesDataset, BirdstextDataset, BillionDataset, FlowerstextDataset\n",
    "from torchvision import transforms\n",
    "import models.text_auto_models1 as text_models\n",
    "from data.resultwriter import ResultWriter\n",
    "from models.utils1 import EarlyStoppingWithOpt, Logger\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from tempfile import mkdtemp\n",
    "from tensorboardX import SummaryWriter\n",
    "data_set_type = \"2\"\n",
    "sys_argv = [\"\", \"2\", \"data output_file.txt\"]\n",
    "###########################################################################\n",
    "###########################################################################\n",
    "####################### variables#######################################\n",
    "##########################################################################\n",
    "########################################################################\n",
    "# Top level data directory\n",
    "data_dir = sys_argv[2]\n",
    "# Flower dataset:1 Birds dataset 2\n",
    "data_set_type = sys_argv[1]\n",
    "# output folder name where results will be saved\n",
    "\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "stored_model_dir='stored_model_dir/'\n",
    "model_name = \"AutoEncoderD\"\n",
    "additional ='glove100_flower'\n",
    "ver='False150'#'True88' #initial version of the model to be loaded\n",
    "glove_folder ='glove.6B_flowers'\n",
    "gname = glove_folder + '/glove.6B.100d.txt' # for glove embedding\n",
    "#gname = None\n",
    "\n",
    "######################################################################\n",
    "################step 1. Run with gname location, benchmark= True#for pretraining the model with glove embedding\n",
    "################step 2. Run with gname location, benchmark=False# for finetuning the pretrained modelwith glove embedding\n",
    "################ if gname location is not given it will be trained from scratch with embedding also trained\n",
    "######################################################################\n",
    "benchmark = False #when True run for benchmark dataset , when false Run for Actual dataset\n",
    "\n",
    "#gname = None\n",
    "if gname is None: # when gname is not provided no benchmark training is done\n",
    "    benchmark= False #because we take first 50k words of glove during benchmark training\n",
    "    \n",
    "# Number of classes in the dataset\n",
    "embedding_dim = 100\n",
    "hidden_dim = 100\n",
    "\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 128\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 500\n",
    "restart_epoch = 152 #1 #49#restartting from failed step\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model, else we only extract features\n",
    "feature_extract = False\n",
    "# Variational Autoencoder turn on\n",
    "var_ae=False\n",
    "#checkpoint_path = \"checkpoints\"\n",
    "# mean and std calculated fort the dataset\n",
    "mean_r= 0\n",
    "mean_g= 0\n",
    "mean_b= 0\n",
    "std_r= 1\n",
    "std_g= 1\n",
    "std_b= 1\n",
    "lr1 = 1e-3\n",
    "\n",
    "##################################################\n",
    "chkpt= stored_model_dir + model_name + additional + ver +'.pt'\n",
    "early_stopping = EarlyStoppingWithOpt(patience=20, verbose=True, checkpoint= chkpt)\n",
    "#########################Seting GPU###########################\n",
    "\n",
    "\n",
    "print(\"Using GPU device\", torch.cuda.current_device())\n",
    "########################################################################\n",
    "######################################################################\n",
    "#########################################################################\n",
    "\n",
    "runId = datetime.datetime.now().isoformat()\n",
    "experiment_dir = Path('experiments/mylogg')\n",
    "experiment_dir.mkdir(parents=True, exist_ok=True)\n",
    "runPath = mkdtemp(prefix=runId, dir=str(experiment_dir))\n",
    "# sys.stdout = Logger('{}/run.log'.format(runPath))\n",
    "print('Expt:', runPath)\n",
    "print('RunID:', runId)\n",
    "# output folder name where results will be saved\n",
    "print('current directory:', os.getcwd())\n",
    "results_writer_val = os.path.join(runPath,'val')\n",
    "if not os.path.exists(results_writer_val):\n",
    "    os.mkdir(results_writer_val)\n",
    "\n",
    "\n",
    "data_transforms = transforms.Compose([transforms.ToPILImage(), transforms.Pad(0), transforms.ToTensor(), transforms.Normalize([mean_r, mean_g, mean_b], [std_r, std_g, std_b])])\n",
    "\n",
    "\n",
    "# data/birds/CUB_200_2011/train_test_split.txt\n",
    "data_dir = \"data/birds\"\n",
    "# Create training and validation data\n",
    "if data_set_type == '1':\n",
    "    text_datasets = {x: FlowerstextDataset(os.path.join(data_dir), transform=data_transforms, split=x) for x in ['train', 'val']}\n",
    "elif data_set_type == '2':\n",
    "    text_datasets = {x: BirdstextDataset(os.path.join(data_dir), transform=data_transforms, split=x) for x in ['train', 'val']}\n",
    "# Create training and validation dataloaders\n",
    "\n",
    "ds = text_datasets['train']\n",
    "\n",
    "vocab = ds.get_vocab_builder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d760bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
