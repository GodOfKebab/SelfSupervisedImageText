{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d7515d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############loading SpacyTokenizer###########\n",
      "PyTorch Version:  2.1.1+cu121\n",
      "Torchvision Version:  0.16.1+cu121\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torchvision\n",
    "import time\n",
    "import sys\n",
    "from data.datasets1 import ShapesDataset, BirdstextDataset, BillionDataset, FlowerstextDataset\n",
    "from torchvision import transforms\n",
    "import models.text_auto_models1 as text_models\n",
    "from data.resultwriter import ResultWriter\n",
    "from models.utils1 import EarlyStoppingWithOpt, Logger\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from tempfile import mkdtemp\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "649f4733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU device 0\n",
      "Expt: experiments/mylogg/2023-12-08T01:32:05.625850r1_wwigw\n",
      "RunID: 2023-12-08T01:32:05.625850\n",
      "current directory: /app\n"
     ]
    }
   ],
   "source": [
    "sys_argv = [\"\", \"2\", \"data output_file.txt\"]\n",
    "###########################################################################\n",
    "###########################################################################\n",
    "####################### variables#######################################\n",
    "##########################################################################\n",
    "########################################################################\n",
    "# Top level data directory\n",
    "data_dir = sys_argv[2]\n",
    "# Flower dataset:1 Birds dataset 2\n",
    "data_set_type = sys_argv[1]\n",
    "# output folder name where results will be saved\n",
    "\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "stored_model_dir='stored_model_dir/'\n",
    "model_name = \"AutoEncoderD\"\n",
    "additional ='glove100_flower'\n",
    "ver='False150'#'True88' #initial version of the model to be loaded\n",
    "glove_folder ='glove.6B_flowers'\n",
    "gname = glove_folder + '/glove.6B.100d.txt' # for glove embedding\n",
    "#gname = None\n",
    "\n",
    "######################################################################\n",
    "################step 1. Run with gname location, benchmark= True#for pretraining the model with glove embedding\n",
    "################step 2. Run with gname location, benchmark=False# for finetuning the pretrained modelwith glove embedding\n",
    "################ if gname location is not given it will be trained from scratch with embedding also trained\n",
    "######################################################################\n",
    "benchmark = False #when True run for benchmark dataset , when false Run for Actual dataset\n",
    "\n",
    "#gname = None\n",
    "if gname is None: # when gname is not provided no benchmark training is done\n",
    "    benchmark= False #because we take first 50k words of glove during benchmark training\n",
    "    \n",
    "# Number of classes in the dataset\n",
    "embedding_dim = 100\n",
    "hidden_dim = 100\n",
    "\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 128\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 500\n",
    "restart_epoch = 1 #1 #49#restartting from failed step\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model, else we only extract features\n",
    "feature_extract = False\n",
    "# Variational Autoencoder turn on\n",
    "var_ae=False\n",
    "#checkpoint_path = \"checkpoints\"\n",
    "# mean and std calculated fort the dataset\n",
    "mean_r= 0\n",
    "mean_g= 0\n",
    "mean_b= 0\n",
    "std_r= 1\n",
    "std_g= 1\n",
    "std_b= 1\n",
    "lr1 = 1e-3\n",
    "\n",
    "##################################################\n",
    "chkpt= stored_model_dir + model_name + additional + ver +'.pt'\n",
    "early_stopping = EarlyStoppingWithOpt(patience=20, verbose=True, checkpoint= chkpt)\n",
    "#########################Seting GPU###########################\n",
    "\n",
    "\n",
    "print(\"Using GPU device\", torch.cuda.current_device())\n",
    "########################################################################\n",
    "######################################################################\n",
    "#########################################################################\n",
    "\n",
    "runId = datetime.datetime.now().isoformat()\n",
    "experiment_dir = Path('experiments/mylogg')\n",
    "experiment_dir.mkdir(parents=True, exist_ok=True)\n",
    "runPath = mkdtemp(prefix=runId, dir=str(experiment_dir))\n",
    "# sys.stdout = Logger('{}/run.log'.format(runPath))\n",
    "print('Expt:', runPath)\n",
    "print('RunID:', runId)\n",
    "# output folder name where results will be saved\n",
    "print('current directory:', os.getcwd())\n",
    "results_writer_val = os.path.join(runPath,'val')\n",
    "if not os.path.exists(results_writer_val):\n",
    "    os.mkdir(results_writer_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaaf13c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#############################################################\n",
    "##############store loss values#################################################\n",
    "loss_dict ={'train_losses': [],'val_losses': []}\n",
    "loss_dict_name= model_name + additional +'_loss_dict.p'\n",
    "writer = SummaryWriter('data1/tensorboard/newmodel_' + model_name+ additional)\n",
    "#######################################################################\n",
    "########################################################################\n",
    "################## Functions and Class defination#########################\n",
    "#####################################################################\n",
    "\n",
    "class SimpleAutoencoder(nn.Module):\n",
    "    #############image autoencoder##############################\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(SimpleAutoencoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, mu, sigma = self.encoder(x)\n",
    "        #x, _, _ = self.encoder(x)\n",
    "        x, _ = self.decoder(x)\n",
    "        return x, mu, sigma\n",
    "\n",
    "def adjust_padding(cap, len1):\n",
    "    cap = cap.numpy()\n",
    "    len1 = len1.numpy()\n",
    "    max_len = max(len1)\n",
    "    temp=[]\n",
    "    for i in cap:\n",
    "        j = i[0:max_len]\n",
    "        temp.append(j)\n",
    "    cap =torch.LongTensor(temp)\n",
    "    len1= torch.LongTensor(len1)\n",
    "    return cap, len1\n",
    "    \n",
    "\n",
    "def load_checkpoint(model, optimizer, filename):\n",
    "    # Note: Input model & optimizer should be pre-defined.  This routine only updates their states.\n",
    "    print(\"=> loading checkpoint '{}'\".format(filename))\n",
    "    checkpoint = torch.load(filename)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    print('model load successful')\n",
    "    return model, optimizer\n",
    "\n",
    "\n",
    "def initialize_model(model_name, config, embeddings_matrix):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    \n",
    "    autoencoder_model= text_models.AutoEncoderD(config, embeddings_matrix)\n",
    "    #decoder = G_NET()\n",
    "\n",
    "\n",
    "    return autoencoder_model\n",
    "\n",
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, restart_epoch=1):\n",
    "    since = time.time()\n",
    "\n",
    "    val_loss_history = []\n",
    "\n",
    "    for epoch in range(restart_epoch-1, num_epochs):\n",
    "        start_t = time.time()\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        if benchmark:\n",
    "            phases = ['train']\n",
    "            save_model_phase = 'train'\n",
    "            print('##############training for benchmark dataset#################')\n",
    "        else:\n",
    "            phases = ['train', 'val']\n",
    "            save_model_phase = 'val'\n",
    "            print('##############training for birds dataset#################')\n",
    "            \n",
    "            \n",
    "        for phase in phases:\n",
    "            #if phase == 'train':\n",
    "             #   model.train()  # Set model to training mode\n",
    "            #else:\n",
    "             #   model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_perplexity = 0.0\n",
    "            \n",
    "                \n",
    "\n",
    "            # Iterate over data.\n",
    "            for _, _, captions, lengths in dataloaders[phase]:\n",
    "                #print('cap:',captions)\n",
    "                #print('len:',lengths)\n",
    "                captions, lengths= adjust_padding(captions, lengths)\n",
    "                #print('new cap:',captions)\n",
    "                #print('new_len:', lengths)\n",
    "                #print('length of dataset',len(dataloaders[phase].dataset))\n",
    "                \n",
    "                captions = captions.cuda()\n",
    "                lengths = lengths.cuda()\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    out, index = model(captions, lengths)\n",
    "                    #print(out.shape)\n",
    "                    #print(captions.shape)\n",
    "                    #print(lengths)\n",
    "                    # Since we train an autoencoder we compare the output to the original input\n",
    "                    loss = criterion(out[:, 1:, :].contiguous().view(-1, out.shape[2]), captions[:, 1:].flatten())\n",
    "                    # backward + optimize only if in training phase\n",
    "                    perplexity  = torch.exp(loss)\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.00)\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                \n",
    "                running_loss += loss.item() * captions.size(0)\n",
    "                running_perplexity += perplexity.item()\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_perplexity = running_perplexity / len(dataloaders[phase])\n",
    "            end_t = time.time()\n",
    "            # calculating perplexity\n",
    "\n",
    "            print('{} Loss: {:.4f} Perplexity: {:.4f}'.format(phase, epoch_loss, epoch_perplexity))\n",
    "            print('time taken:', end_t - start_t)\n",
    "\n",
    "            # deep copy the model\n",
    "            \n",
    "            if phase == save_model_phase:\n",
    "                ################checking intermediate results################\n",
    "                f =open(os.path.join(results_writer_val, 'result_epoch_'+str(epoch)+'.txt'), 'w')\n",
    "                texts_i = vocab.decode_positions(captions)\n",
    "                texts_o = vocab.decode_positions(index)\n",
    "                for l, o in zip(texts_i, texts_o):\n",
    "                    print(l,'\\t',o, file = f)\n",
    "                f.close()\n",
    "                ##########################################################\n",
    "                ver = str(benchmark) + str(epoch)\n",
    "                chkpt= stored_model_dir + model_name + additional + ver +'.pt'\n",
    "                early_stopping(epoch_loss, model, optimizer, chkpt)\n",
    "                val_loss_history.append(epoch_loss)\n",
    "                \n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val loss: {:4f}'.format(min(val_loss_history)))\n",
    "    # load best model weights\n",
    "    ver = str(benchmark) + str(epoch- early_stopping.counter)\n",
    "    chkpt= stored_model_dir + model_name + additional + ver +'.pt'\n",
    "    model, optimizer = load_checkpoint(model, optimizer, chkpt)\n",
    "    #model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "    return model, val_loss_history\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "356201d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([transforms.ToPILImage(), transforms.Pad(0), transforms.ToTensor(), transforms.Normalize([mean_r, mean_g, mean_b], [std_r, std_g, std_b])])\n",
    "inv_normalize = transforms.Normalize(mean=[-mean_r/std_r, -mean_g/std_g, -mean_b/std_b],std=[1/std_r, 1/std_g, 1/std_b])\n",
    "# Data augmentation and normalization for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f212789d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BillionDataset] (1.00%=10001/1000000) Loading Captions To an Array... \n",
      "[BillionDataset] (2.00%=20002/1000000) Loading Captions To an Array... \n",
      "[BillionDataset] (3.00%=30003/1000000) Loading Captions To an Array... \n",
      "[BillionDataset] (4.00%=40004/1000000) Loading Captions To an Array... \n",
      "[BillionDataset] (5.00%=50005/1000000) Loading Captions To an Array... \n"
     ]
    }
   ],
   "source": [
    "#pretrain_dir = '1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/*'\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "pretrain_dir = 'data/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/*'\n",
    "benchmark_datasets = {x: BillionDataset(pretrain_dir, split=x) for x in ['train']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828f3716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys, importlib\n",
    "# importlib.reload(sys.modules['data.datasets1'])\n",
    "# from data.datasets1 import BirdstextDataset\n",
    "\n",
    "# data/birds/CUB_200_2011/train_test_split.txt\n",
    "data_dir = \"data/birds\"\n",
    "# Create training and validation data\n",
    "if data_set_type == '1':\n",
    "    text_datasets = {x: FlowerstextDataset(os.path.join(data_dir), transform=data_transforms, split=x) for x in ['train', 'val']}\n",
    "elif data_set_type == '2':\n",
    "    text_datasets = {x: BirdstextDataset(os.path.join(data_dir), transform=data_transforms, split=x) for x in ['train']}\n",
    "# Create training and validation dataloaders\n",
    "\n",
    "ds = text_datasets['train']\n",
    "\n",
    "vocab = ds.get_vocab_builder()\n",
    "#print('the max length is', ds.max_sent_length)\n",
    "#print('the vocab size of birds dataset', vocab.vocab_size())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68ce786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_benchmark_datasets = open(\"benchmark_datasets.pickle\", 'wb')\n",
    "# pickle.dump(benchmark_datasets, file_benchmark_datasets)\n",
    "# file_benchmark_datasets.close()\n",
    "\n",
    "# file_vocab = open(\"vocab.pickle\", 'wb')\n",
    "# pickle.dump(vocab, file_vocab)\n",
    "# file_vocab.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba16e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "glove_folder ='data/glove'\n",
    "gname = glove_folder + '/glove.6B.100d.txt' # for glove embedding\n",
    "# gname = None\n",
    "#print(os.getcwd())\n",
    "if gname is not None:\n",
    "    top50k =[]\n",
    "    t2i= {}\n",
    "    ###########################Loading################################\n",
    "    file_ematrix = open(os.path.join(glove_folder,'emtrix.obj'), 'rb') \n",
    "    embeddings_matrix = pickle.load(file_ematrix)\n",
    "    \n",
    "    file_vocab_i2t = open(os.path.join(glove_folder,'vocab_i2t.obj'), 'rb')\n",
    "    vocab.i2t = pickle.load(file_vocab_i2t)\n",
    "    \n",
    "    file_vocab_t2i = open(os.path.join(glove_folder,'vocab_t2i.obj'), 'rb')\n",
    "    vocab.t2i = pickle.load(file_vocab_t2i)\n",
    "    \n",
    "    print('embedding matrix, vocab.i2t, vocab.t2i are saved at ', file_ematrix.name, file_vocab_i2t.name, file_vocab_t2i.name)\n",
    "#     text_datasets['val'].vocab_builder.i2t =vocab.i2t\n",
    "    #f1 =open('i2t', 'w+')\n",
    "    #print('benchmark dataset previous vocabsize', benchmark_datasets['train'].vocab_builder.i2t, file=f1)\n",
    "    #print('length of emtarix', len(embeddings_index))\n",
    "    #for i, word in enumerate(benchmark_datasets['train'].vocab_builder.i2t):\n",
    "     #   embedding_vector = embeddings_index.get(word)\n",
    "      #  if embedding_vector is None:\n",
    "       #     print(word)\n",
    "        \n",
    "        \n",
    "    benchmark_datasets['train'].vocab_builder.i2t = vocab.i2t\n",
    "    #print('benchmark dataset new vocabsize', len(benchmark_datasets['train'].vocab_builder.i2t))\n",
    "#     text_datasets['val'].vocab_builder.t2i = vocab.t2i\n",
    "    benchmark_datasets['train'].vocab_builder.t2i = vocab.t2i\n",
    "    #print('length of t2i', len(benchmark_datasets['train'].vocab_builder.t2i), len(t2i))\n",
    "    \n",
    "    vocab = ds.get_vocab_builder()\n",
    "    #print('compare vocubbuilder birds', ds.vocab_builder.vocab_size(), text_datasets['train'].vocab_builder.vocab_size())\n",
    "\n",
    "else:\n",
    "    embeddings_matrix = None\n",
    "\n",
    "    \n",
    "print(f\"{embeddings_matrix.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f76739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import torch\n",
    "# # import torch.nn as nn\n",
    "# import importlib\n",
    "# importlib.reload(text_models)\n",
    "# import torch.nn as nn\n",
    "# # print(vocab.pad_pos())\n",
    "# nn.Embedding(400000, 100, padding_idx=0)\n",
    "# torch.cuda.FloatTensor(embeddings_matrix)\n",
    "# import models.text_auto_models1 as text_models\n",
    "\n",
    "\n",
    "# def initialize_model(model_name, config, embeddings_matrix):\n",
    "#     # Initialize these variables which will be set in this if statement. Each of these\n",
    "#     #   variables is model specific.\n",
    "    \n",
    "#     autoencoder_model= text_models.AutoEncoderD(config, embeddings_matrix)\n",
    "#     #decoder = G_NET()\n",
    "\n",
    "\n",
    "#     return autoencoder_model\n",
    "\n",
    "# # torch.set_default_device('cuda')\n",
    "\n",
    "# def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, restart_epoch=1):\n",
    "#     since = time.time()\n",
    "\n",
    "#     val_loss_history = []\n",
    "\n",
    "#     for epoch in range(restart_epoch-1, num_epochs):\n",
    "#         start_t = time.time()\n",
    "#         print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "#         print('-' * 10)\n",
    "\n",
    "#         # Each epoch has a training and validation phase\n",
    "#         if benchmark:\n",
    "#             phases = ['train']\n",
    "#             save_model_phase = 'train'\n",
    "#             print('##############training for benchmark dataset#################')\n",
    "#         else:\n",
    "#             phases = ['train', 'val']\n",
    "#             save_model_phase = 'val'\n",
    "#             print('##############training for birds dataset#################')\n",
    "            \n",
    "            \n",
    "#         for phase in phases:\n",
    "#             #if phase == 'train':\n",
    "#              #   model.train()  # Set model to training mode\n",
    "#             #else:\n",
    "#              #   model.eval()   # Set model to evaluate mode\n",
    "\n",
    "#             running_loss = 0.0\n",
    "#             running_perplexity = 0.0\n",
    "            \n",
    "                \n",
    "\n",
    "#             # Iterate over data.\n",
    "#             for _, _, captions, lengths in dataloaders[phase]:\n",
    "#                 #print('cap:',captions)\n",
    "#                 #print('len:',lengths)\n",
    "#                 captions, lengths= adjust_padding(captions, lengths)\n",
    "#                 #print('new cap:',captions)\n",
    "#                 #print('new_len:', lengths)\n",
    "#                 #print('length of dataset',len(dataloaders[phase].dataset))\n",
    "                \n",
    "#                 captions = captions.cuda()\n",
    "#                 lengths = lengths.cuda()\n",
    "#                 # zero the parameter gradients\n",
    "#                 optimizer.zero_grad()\n",
    "\n",
    "#                 # forward\n",
    "#                 # track history if only in train\n",
    "#                 with torch.set_grad_enabled(phase == 'train'):\n",
    "#                     # Get model outputs and calculate loss\n",
    "#                     out, index = model(captions, lengths)\n",
    "#                     #print(out.shape)\n",
    "#                     #print(captions.shape)\n",
    "#                     #print(lengths)\n",
    "#                     # Since we train an autoencoder we compare the output to the original input\n",
    "#                     loss = criterion(out[:, 1:, :].contiguous().view(-1, out.shape[2]), captions[:, 1:].flatten())\n",
    "#                     # backward + optimize only if in training phase\n",
    "#                     perplexity  = torch.exp(loss)\n",
    "#                     if phase == 'train':\n",
    "#                         loss.backward()\n",
    "#                         torch.nn.utils.clip_grad_norm_(model.parameters(), 5.00)\n",
    "#                         optimizer.step()\n",
    "\n",
    "#                 # statistics\n",
    "                \n",
    "#                 running_loss += loss.item() * captions.size(0)\n",
    "#                 running_perplexity += perplexity.item()\n",
    "\n",
    "#             epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "#             epoch_perplexity = running_perplexity / len(dataloaders[phase])\n",
    "#             end_t = time.time()\n",
    "#             # calculating perplexity\n",
    "\n",
    "#             print('{} Loss: {:.4f} Perplexity: {:.4f}'.format(phase, epoch_loss, epoch_perplexity))\n",
    "#             print('time taken:', end_t - start_t)\n",
    "\n",
    "#             # deep copy the model\n",
    "            \n",
    "#             if phase == save_model_phase:\n",
    "#                 ################checking intermediate results################\n",
    "#                 f =open(os.path.join(results_writer_val, 'result_epoch_'+str(epoch)+'.txt'), 'w')\n",
    "#                 texts_i = vocab.decode_positions(captions)\n",
    "#                 texts_o = vocab.decode_positions(index)\n",
    "#                 for l, o in zip(texts_i, texts_o):\n",
    "#                     print(l,'\\t',o, file = f)\n",
    "#                 f.close()\n",
    "#                 ##########################################################\n",
    "#                 ver = str(benchmark) + str(epoch)\n",
    "#                 chkpt= stored_model_dir + model_name + additional + ver +'.pt'\n",
    "#                 early_stopping(epoch_loss, model, optimizer, chkpt)\n",
    "#                 val_loss_history.append(epoch_loss)\n",
    "                \n",
    "#         if early_stopping.early_stop:\n",
    "#             print(\"Early stopping\")\n",
    "#             break\n",
    "\n",
    "#         print()\n",
    "\n",
    "#     time_elapsed = time.time() - since\n",
    "#     print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "#     print('Best val loss: {:4f}'.format(min(val_loss_history)))\n",
    "#     # load best model weights\n",
    "#     ver = str(benchmark) + str(epoch- early_stopping.counter)\n",
    "#     chkpt= stored_model_dir + model_name + additional + ver +'.pt'\n",
    "#     model, optimizer = load_checkpoint(model, optimizer, chkpt)\n",
    "#     #model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "#     return model, val_loss_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11574b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bench_dataloader_dict = {x: DataLoader(benchmark_datasets[x], batch_size=batch_size, shuffle=True, num_workers=0, generator=torch.Generator(device='cuda'),) for x in ['train']}  \n",
    "dataloaders_dict = {x: DataLoader(text_datasets[x], batch_size=batch_size, shuffle=True, num_workers=0, generator=torch.Generator(device='cuda'),) for x in ['train']}\n",
    "\n",
    "# import models.text_auto_models1 as text_models\n",
    "\n",
    "config = {  'emb_dim': embedding_dim,\n",
    "            'vocab_size': embeddings_matrix.shape[0],\n",
    "                'hid_dim': hidden_dim//2, #birectional is used so hidden become double\n",
    "                'n_layers': 1,\n",
    "                'dropout': 0.0,\n",
    "                'vocab_size': vocab.vocab_size(),\n",
    "                'sos': vocab.sos_pos(),\n",
    "                'eos': vocab.eos_pos(),\n",
    "                'pad': vocab.pad_pos(),\n",
    "             }\n",
    "print(f\"{config=}\")\n",
    "#print('pad position', vocab.pad_pos())\n",
    "# Initialize the model for this run() second one if want to load weights from previous check point\n",
    "model_ft = initialize_model(model_name, config, embeddings_matrix)\n",
    "#model_ft, input_size = initialize_model(model_name, feature_vector_dim, feature_extract, use_pretrained=True, vae=var_ae, use_finetuned='checkpoint.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d9b879",
   "metadata": {},
   "outputs": [],
   "source": [
    "restart_epoch = 1\n",
    "benchmark= True\n",
    "# print(((not benchmark) and gname is not None))\n",
    "# print(dataloaders_dict['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30e862b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model we just instantiated\n",
    "print(model_ft)\n",
    "#model_ft = torch.nn.DataParallel(model_ft)\n",
    "# Send the model to GPU\n",
    "model_ft.cuda()\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are\n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.Adam(params_to_update)\n",
    "\n",
    "\n",
    "if (restart_epoch > 1) or ((not benchmark) and gname is not None):\n",
    "        print('loading model from checkpoint............')\n",
    "        model_ft, optimizer = load_checkpoint(model_ft, optimizer_ft, chkpt)\n",
    "        params_to_update = model_ft.parameters()\n",
    "        optimizer_ft = optim.Adam(params_to_update)\n",
    "        if (restart_epoch > 1): # when restarting both optimizer and model is loaded\n",
    "            print('loading optimizer from checkpoint.................')\n",
    "            optimizer_ft = optimizer\n",
    "if not benchmark:\n",
    "    dataloaders_dict = dataloaders_dict\n",
    "    output_loader = dataloaders_dict['val']\n",
    "else:\n",
    "    dataloaders_dict = bench_dataloader_dict\n",
    "    output_loader = dataloaders_dict['train']\n",
    "    \n",
    "\n",
    "# Setup the loss fxn\n",
    "#criterion = loss_function\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=vocab.pad_pos())\n",
    "if torch.cuda.is_available():\n",
    "    criterion.cuda()\n",
    "    \n",
    "# Train and evaluate\n",
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, restart_epoch=restart_epoch)\n",
    "#torch.save(model_ft.state_dict(), os.path.join(checkpoint_path, \"pytorch_model.bin\"))\n",
    "\n",
    "# show reconstruction for first batch\n",
    "model_ft.eval() #set eval mode for testing\n",
    "#print('#################Eval mode##################')\n",
    "f =open(os.path.join(results_writer_val, sys_argv[3]), 'w')\n",
    "print('actual','\\t','generated', file = f)\n",
    "for _, _, cap, len1 in output_loader:\n",
    "    cap, len1= adjust_padding(cap, len1)\n",
    "    cap = cap.cuda()\n",
    "    len1 = len1.cuda()\n",
    "    _, ind = model_ft(cap, len1)\n",
    "    texts_i = vocab.decode_positions(cap)\n",
    "    texts_o = vocab.decode_positions(ind)\n",
    "    for l, o in zip(texts_i, texts_o):\n",
    "        print(l,'\\t',o, file = f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2b389e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c70db8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
